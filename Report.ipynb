{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Ear Recognition\n",
    "Yifei Feng | U81901533\n",
    "\n",
    "Zehui Jiang | U68975915\n",
    "\n",
    "Guangxing Ren | U072735315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract:\n",
    "**The process of precisely recognize people by ears has been getting major attention in recent years. It represents an important step in the biometric research, especially as a complement to face recognition systems which have difficult in real conditions. This is due to the great variation in shapes, variable lighting conditions, and the changing profile shape which is a planar representation of a complex object. We present an ear recognition system involving a convolutional neural networks (CNN)  to identify a person given an input image.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify your problem\n",
    "In this project, we are given a set of left ear image from people with different identities. For each ear(each person). Four images are given, we for ears “in the wild” (there is no constraint in the way of taking the photo), two for ear images taken with a “donut device” that serves as a background and somewhat controls lighting.  There are 4*195(individual ear) in the dataset. <Br>\n",
    "    (Data source: http://cs-people.bu.edu/wdqin/earImageDataset.zip)\n",
    "\n",
    "The term ear biometrics refers to automatic human identification on the basis of the ear physiological (anatomical) features. The identification is performed on the basis of the features which are usually calculated from captured 2D or 3D ear images (using pattern recognition and image processing techniques). \n",
    "\n",
    "Being able to identify human ears automatically has a significant meaning. It can be useful in several domains such as:\n",
    "<li>1. Criminal determination\n",
    "<li>2. Enfant identification\n",
    "<li>3. Medical research\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a classification problem with 195 categories and in each category there are 4 sample images. Our goal is: given a random ear image in this dataset,  being able to identify which person this ear belongs to and display all the other ears belong to this person.**<Br>\n",
    "    \n",
    "This is no minor problem, which involves several steps:\n",
    "<li>Choose a proper method after researching and comparison between all possible method\n",
    "<li>Building a model with the chosen method\n",
    "<li>Input all the data\n",
    "<li>Train a model that is capable of identifying all images with a rather high accuracy, higher than 0.8 preferably\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Survey\n",
    "\n",
    "\n",
    "## Possible Solutions:\n",
    "\n",
    "<li>1. Neural Network\n",
    "<li>2. Hidden Markov Model\n",
    "<li>3. Edge Detection\n",
    "\n",
    "## Choice of Method\n",
    "    Our final choice is Neural network, to be specific, Convolutional Neural Network for the following reasons:\n",
    "<li>HMM is a generative, probabilistical model. It’s a generative, probabilistic model where you try to model the process generating the training sequences, or more precisely, the distribution over the sequences of observations. But in our case, we are trying to categorize hundreds of images and CNN, as a deterministic model, will be a better fit.\n",
    "<li>Edge detection can be used to extract the feature of the shape of ears. But speaking of identifying and categorizing them, it lacks abundance in feature information. \n",
    "<li>CNN is widely used in image concerning problems such as image classification, image detection and image generating, etc.\n",
    "\n",
    "## About CNN\n",
    "### How CNN works:\n",
    "<p>CNN is widely used for image recognition for a long period of time. There are various well-developed network structure and framework/platform to utilize. This is the main reason why we choose CNN as a solution.\n",
    "Convolutional neuron network is mainly consist of convolutional layer, pooling layer and fully connected layer.\n",
    "Convolutional layer basically take filters on a single image and each filter picks different singal(area of the picture) in hortional, vertical and diagonal directions.\n",
    "<img src=\"./gif/1.gif\"> <br>    \n",
    "The aim of those filters is creating a map of each slices in image that feature occurs.  So convolutional networks perform a sort of search. During search, a match is found, it will be mapped into a feature space where location of the match will be saved. By repeating above steps, convolutional layer can record features of the single image in different directions. After convolutional layer, it might be passed into a nonlinear transform such as reLu or tanh, which compress input into a range between 1 and -1.\n",
    "<img src=\"./gif/2.gif\"> <br> \n",
    "Pooling layer has various kinds, such as maxPooling, average and downsample. In pooling layer, maps will be applied a slice/patch once a time. For example, maxPooling will take the largest value in the slice/patch and discard other information in maps. It is kind of compressing maps into smaller dimensions and \n",
    "save key feature in the same time.\n",
    "Fully connected layer will classify output on each node based on weights.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we choose Keras:\n",
    "Keras is chosen as a developing framework for this project. keras is well developed and user friendly for starters. It provides various APIs. As a result, we can more focus on how to improve our network in high level rather than in debugging our network structure. . Keras is indeed more readable and concise, allowing us to build first end-to-end deep learning models faster, while skipping the implementation details .Kears is built on top of Tensorflow, which is widely used for deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Reproducing the baseline:\n",
    "\n",
    "**We find a existing project of face recognition based on Keras, which is similar to our project in some way. However, there is still a big difference between it and our work. So we use its network frame as a reference and build our own convolutional network with 2 convolutional layers, 2 pooling layers and 1 fully connected layer.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from skimage.io import imread_collection\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.utils import np_utils \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#loading dataset from directory and resize every picture to 200x200 dmension\n",
    "def load_data(path, size):\n",
    "    #creating a collection with the available images\n",
    "    image = imread_collection(path)\n",
    "    image_set = []\n",
    "    for n in image:\n",
    "        n = cv2.cvtColor(n,cv2.COLOR_RGB2GRAY)\n",
    "        n = cv2.resize(n,(size,size)) \n",
    "        n = n / 255\n",
    "        image_set.append(n)\n",
    "    return image_set \n",
    "    \n",
    "def set_init(dataset, train_set_ratio, valid_set_ratio, test_set_ratio):\n",
    "\n",
    "    \n",
    "    #creating label set for all images\n",
    "    label = np.empty(195*4)\n",
    "    for i in range(195):\n",
    "        label[i*4:i*4+4] = i\n",
    "    label = label.astype(np.int)\n",
    "    label = np_utils.to_categorical(label, 195)#transfer to one-hot matrix\n",
    "    \n",
    "    train_num = 780*train_set_ratio\n",
    "    train_num = int(train_num)\n",
    "    valid_num = 780*valid_set_ratio\n",
    "    valid_num = int(valid_num)\n",
    "    test_num = 780*test_set_ratio\n",
    "    test_num = int(test_num) \n",
    "    \n",
    "    train_data = np.empty((train_num,200,200))  #creating numpy array for different datasets\n",
    "    train_label = np.empty((train_num,195))   \n",
    "    valid_data = np.empty((valid_num, 200,200))   \n",
    "    valid_label = np.empty((valid_num,195))   \n",
    "    test_data = np.empty((test_num,200,200))  \n",
    "    test_label = np.empty((test_num,195)) \n",
    "    \n",
    "    x_test_tot = np.empty((valid_num + test_num,200,200))\n",
    "    y_test_tot = np.empty((valid_num + test_num,195))\n",
    "    \n",
    "    #split into train set and validation-test set\n",
    "    train_data, x_test_tot, train_label, y_test_tot = train_test_split(dataset, label, test_size = 1-train_set_ratio)\n",
    "    \n",
    "    #split validation-test set into validation and test set\n",
    "    valid_data, test_data, valid_label, test_label = train_test_split(x_test_tot, y_test_tot, test_size = test_set_ratio/(valid_set_ratio + test_set_ratio))\n",
    "    \n",
    "    train_data = np.asarray(train_data)\n",
    "    x_test_tot = np.asarray(x_test_tot)\n",
    "    valid_data = np.asarray(valid_data)\n",
    "    test_data = np.asarray(test_data)\n",
    "    result = [(train_data, train_label), (valid_data, valid_label),(test_data, test_label)]\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = load_data('original/*.jpg',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = set_init(data_set, 0.8, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from PIL import Image\n",
    "def train(data, batch_size, epochs, nb_filters, pool_size, kernel_size):\n",
    "    np.random.seed(1337)  # for reproducibility\n",
    "    img_rows, img_cols = 200, 200  # width and height of pictures\n",
    "    nb_classes = 195  # number of classes\n",
    "    input_shape = (img_rows, img_cols,1)  # dimenstion\n",
    "\n",
    "    [(X_train, Y_train), (X_valid, Y_valid),(X_test, Y_test)] = data\n",
    "\n",
    "    X_train = X_train[:,:,:,np.newaxis]  # add one dimenstion, keras required. total 4 dimension.\n",
    "    X_valid=X_valid[:,:,:,np.newaxis]  \n",
    "    X_test=X_test[:,:,:,np.newaxis]  \n",
    "    print('dimension of train set：', X_train.shape,Y_train.shape)\n",
    "    print('dimension of test set：', X_test.shape,Y_test.shape)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6,kernel_size,input_shape=input_shape,strides=1))  # convolution layer 1\n",
    "    model.add(AveragePooling2D(pool_size,strides=2))  # pooling layer\n",
    "    model.add(Conv2D(12,kernel_size,strides=1))  # convolution layer 2\n",
    "    model.add(AveragePooling2D(pool_size,strides=2))  # pooling layer\n",
    "    model.add(Flatten())  # 1 denmension\n",
    "    model.add(Dense(nb_classes))  # fully connected layer\n",
    "    model.add(Activation('sigmoid'))  # \n",
    "\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\n",
    "    # fit\n",
    "    model.fit(X_train, Y_train, batch_size, epochs,verbose=1, validation_data=(X_valid, Y_valid))\n",
    "    # evaluate\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "    #predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = y_pred.argmax(axis=1)   # check which class is predicted \n",
    "    for i in range(len(y_pred)):\n",
    "    #     oneimg = X_test[i,:,:,0]*256\n",
    "    #     im = Image.fromarray(oneimg)\n",
    "    #     im.show()\n",
    "        print('%d person is predicted as %dperson'%(Y_test.argmax(axis=1).item(i),y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数据集的维度： (624, 200, 200, 1) (624, 195)\n",
      "测试数据集的维度： (78, 200, 200, 1) (78, 195)\n",
      "Train on 624 samples, validate on 78 samples\n",
      "Epoch 1/30\n",
      "624/624 [==============================] - 1s 1ms/step - loss: 5.4536 - acc: 0.0048 - val_loss: 5.2910 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "624/624 [==============================] - 1s 890us/step - loss: 5.2756 - acc: 0.0176 - val_loss: 5.4804 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "624/624 [==============================] - 1s 921us/step - loss: 5.2198 - acc: 0.0385 - val_loss: 5.4052 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "624/624 [==============================] - 1s 926us/step - loss: 5.1425 - acc: 0.0769 - val_loss: 5.5298 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "624/624 [==============================] - 1s 914us/step - loss: 5.0097 - acc: 0.1218 - val_loss: 5.9510 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "624/624 [==============================] - 1s 882us/step - loss: 4.8715 - acc: 0.1282 - val_loss: 5.8096 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "624/624 [==============================] - 1s 869us/step - loss: 4.6691 - acc: 0.1859 - val_loss: 6.1287 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "624/624 [==============================] - 1s 872us/step - loss: 4.3007 - acc: 0.2340 - val_loss: 7.6088 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "624/624 [==============================] - 1s 952us/step - loss: 4.2188 - acc: 0.2821 - val_loss: 6.2276 - val_acc: 0.0128\n",
      "Epoch 10/30\n",
      "624/624 [==============================] - 1s 940us/step - loss: 4.0660 - acc: 0.3686 - val_loss: 7.9962 - val_acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "624/624 [==============================] - 1s 947us/step - loss: 3.1928 - acc: 0.4776 - val_loss: 5.5994 - val_acc: 0.0128\n",
      "Epoch 12/30\n",
      "624/624 [==============================] - 1s 921us/step - loss: 3.3628 - acc: 0.4679 - val_loss: 7.7965 - val_acc: 0.0256\n",
      "Epoch 13/30\n",
      "624/624 [==============================] - 1s 886us/step - loss: 2.4455 - acc: 0.6330 - val_loss: 8.4654 - val_acc: 0.0128\n",
      "Epoch 14/30\n",
      "624/624 [==============================] - 1s 883us/step - loss: 1.4953 - acc: 0.6891 - val_loss: 6.8011 - val_acc: 0.0256\n",
      "Epoch 15/30\n",
      "624/624 [==============================] - 1s 864us/step - loss: 1.5865 - acc: 0.7404 - val_loss: 10.0958 - val_acc: 0.0128\n",
      "Epoch 16/30\n",
      "624/624 [==============================] - 1s 952us/step - loss: 0.8300 - acc: 0.8333 - val_loss: 10.2083 - val_acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "624/624 [==============================] - 1s 890us/step - loss: 0.7638 - acc: 0.8814 - val_loss: 10.4910 - val_acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "624/624 [==============================] - 1s 874us/step - loss: 0.6151 - acc: 0.8990 - val_loss: 8.8080 - val_acc: 0.1282\n",
      "Epoch 19/30\n",
      "624/624 [==============================] - 1s 900us/step - loss: 0.0781 - acc: 0.9872 - val_loss: 8.9248 - val_acc: 0.1282\n",
      "Epoch 20/30\n",
      "624/624 [==============================] - 1s 933us/step - loss: 0.0444 - acc: 0.9920 - val_loss: 8.6891 - val_acc: 0.1410\n",
      "Epoch 21/30\n",
      "624/624 [==============================] - 1s 920us/step - loss: 0.0335 - acc: 0.9952 - val_loss: 8.6782 - val_acc: 0.1410\n",
      "Epoch 22/30\n",
      "624/624 [==============================] - 1s 915us/step - loss: 0.0235 - acc: 0.9984 - val_loss: 8.7004 - val_acc: 0.1410\n",
      "Epoch 23/30\n",
      "624/624 [==============================] - 1s 884us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 8.7436 - val_acc: 0.1410\n",
      "Epoch 24/30\n",
      "624/624 [==============================] - 1s 874us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 8.7507 - val_acc: 0.1410\n",
      "Epoch 25/30\n",
      "624/624 [==============================] - 1s 947us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 8.7723 - val_acc: 0.1410\n",
      "Epoch 26/30\n",
      "624/624 [==============================] - 1s 946us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 8.8143 - val_acc: 0.1410\n",
      "Epoch 27/30\n",
      "624/624 [==============================] - 1s 949us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 8.8238 - val_acc: 0.1410\n",
      "Epoch 28/30\n",
      "624/624 [==============================] - 1s 969us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 8.8462 - val_acc: 0.1410\n",
      "Epoch 29/30\n",
      "624/624 [==============================] - 1s 999us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 8.8682 - val_acc: 0.1410\n",
      "Epoch 30/30\n",
      "624/624 [==============================] - 1s 963us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 8.8905 - val_acc: 0.1410\n",
      "Test score: 8.397333414126665\n",
      "Test accuracy: 0.16666666781290984\n",
      "第36个人识别为第67个人\n",
      "第2个人识别为第158个人\n",
      "第104个人识别为第63个人\n",
      "第105个人识别为第38个人\n",
      "第137个人识别为第146个人\n",
      "第124个人识别为第152个人\n",
      "第20个人识别为第178个人\n",
      "第180个人识别为第180个人\n",
      "第172个人识别为第47个人\n",
      "第184个人识别为第184个人\n",
      "第27个人识别为第27个人\n",
      "第35个人识别为第159个人\n",
      "第189个人识别为第50个人\n",
      "第82个人识别为第82个人\n",
      "第48个人识别为第39个人\n",
      "第24个人识别为第174个人\n",
      "第143个人识别为第27个人\n",
      "第14个人识别为第173个人\n",
      "第64个人识别为第64个人\n",
      "第19个人识别为第18个人\n",
      "第151个人识别为第179个人\n",
      "第131个人识别为第92个人\n",
      "第33个人识别为第114个人\n",
      "第50个人识别为第190个人\n",
      "第39个人识别为第186个人\n",
      "第0个人识别为第54个人\n",
      "第187个人识别为第76个人\n",
      "第35个人识别为第77个人\n",
      "第45个人识别为第99个人\n",
      "第35个人识别为第28个人\n",
      "第3个人识别为第145个人\n",
      "第55个人识别为第147个人\n",
      "第69个人识别为第182个人\n",
      "第8个人识别为第100个人\n",
      "第179个人识别为第71个人\n",
      "第4个人识别为第82个人\n",
      "第49个人识别为第153个人\n",
      "第21个人识别为第21个人\n",
      "第96个人识别为第168个人\n",
      "第89个人识别为第92个人\n",
      "第46个人识别为第11个人\n",
      "第80个人识别为第69个人\n",
      "第13个人识别为第108个人\n",
      "第169个人识别为第71个人\n",
      "第60个人识别为第178个人\n",
      "第15个人识别为第87个人\n",
      "第104个人识别为第33个人\n",
      "第86个人识别为第169个人\n",
      "第151个人识别为第179个人\n",
      "第139个人识别为第39个人\n",
      "第96个人识别为第158个人\n",
      "第113个人识别为第174个人\n",
      "第43个人识别为第51个人\n",
      "第60个人识别为第63个人\n",
      "第31个人识别为第5个人\n",
      "第143个人识别为第79个人\n",
      "第138个人识别为第138个人\n",
      "第106个人识别为第106个人\n",
      "第57个人识别为第57个人\n",
      "第110个人识别为第110个人\n",
      "第111个人识别为第37个人\n",
      "第146个人识别为第146个人\n",
      "第104个人识别为第17个人\n",
      "第75个人识别为第66个人\n",
      "第97个人识别为第97个人\n",
      "第107个人识别为第153个人\n",
      "第138个人识别为第72个人\n",
      "第59个人识别为第3个人\n",
      "第61个人识别为第153个人\n",
      "第30个人识别为第108个人\n",
      "第98个人识别为第98个人\n",
      "第81个人识别为第168个人\n",
      "第175个人识别为第179个人\n",
      "第84个人识别为第45个人\n",
      "第185个人识别为第142个人\n",
      "第46个人识别为第17个人\n",
      "第109个人识别为第77个人\n",
      "第90个人识别为第68个人\n"
     ]
    }
   ],
   "source": [
    "train(data, 100, 30, 64, 4, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement\n",
    "### 1. Crop \n",
    "### 2. Regularization and dropout\n",
    "### 3. K-fold\n",
    "### 4. Augmentation\n",
    "### 5. Final Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we further observe the images, we found out that the noise of a huge portion of images is very huge. So we cropped each picture so that what left in each image has \"more\" ears and less noise such as hair, neck, donut device,etc.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "<img src=\"./screenshot/corp/modify.png\"> <br>\n",
    "### And this improvement is implemented in all parts below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization and dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L2 regularization** is a technique we are going to discuss in more details. Simply put, it introduces a cost term for bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero and hence reduce cost term.\n",
    "\n",
    "**Dropout** is implemented per-layer in a neural network. It can be used with most types of layers, such as dense fully connected layers, convolutional layers, and recurrent layers such as the long short-term memory network layer. Dropout may be implemented on any or all hidden layers in the network as well as the visible or input layer. It is not used on the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "<img src=\"./screenshot/l2drop/modify.png\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for Regularization\n",
    "<img src=\"./screenshot/l2drop/result.png\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "### L2 regularization works significantly for common neural networks, but not good enough for CNN. I believe it's becasue the nature of CNN. For CNN, unlike common neural networks, back propagation relies on chain rule in a linear fashion. But for CNN, not all layers are fully connected until the fully connected layer. So the reduction in gradient decende is not as efficient.\n",
    "### Dropout is generally less effective at regularizing convolutional layers. Since convolutional layers have few parameters, they need less regularization to begin with. Furthermore, because of the spatial relationships encoded in feature maps, activations can become highly correlated. This renders dropout ineffective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-fold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How to Implement:\n",
    "<li>Shuffle the dataset randomly.\n",
    "<li>Split the dataset into k groups\n",
    "<li>For each unique group:\n",
    "    Take the group as a hold out or test data set; \n",
    "    Take the remaining groups as a training data set; \n",
    "    Fit a model on the training set and evaluate it on the test set; \n",
    "    Retain the evaluation score and discard the model; \n",
    "<li>Summarize the skill of the model using the sample of model evaluation scores\n",
    "<img src=\"./screenshot/kfold/modify.png\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For k-fold, our number of fold is 2. During the process of spliting data into training and test set, we decided to use 2 pictures in traning, 1 pictures in validation, 1 pictures in test in random order. Otherwise, it will be compiled error because we have to make sure every class has at least one picture in test set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for fold No.1\n",
    "<img src=\"./screenshot/kfold/fold1.png\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for fold No.2\n",
    "<img src=\"./screenshot/kfold/fold1.png\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "### K-fold doesn't improve our result because cross-validation mainly reduce the inner bias within the dataset. But in our case, as the source of image is very limited, K-fold don't have a huge impact on improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The main problem from baseline is overfitting. As a result, we are trying to utlize data augmentation to provide more traning data. So we use Image Generator provided in keras. We decided to use zoom and rotation. Other parameters are not effiecent in our input dataset. Or our memory(16GB) is not enough to support this modification.** \n",
    "### Implementation\n",
    "<img src=\"./screenshot/augmentation/modify1.png\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "<img src=\"./screenshot/augmentation/result.png\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can discover, there is a some improvement in test accuracy. But still, it's not at all a positive result as a test result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the improvement method we implemented, the problem still exists. However, we are rather satisfied that our network works very well on the training set. So, in order to solve this problem and achieve our goal, we come up with a solution anyway, with is our compromise: **we take the whole data set as the train set**. In this case, **all pictures can be identified and categorized**. What's more, **given a random image in this image set, we are able to display all other ear images that belong to the same person**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "<img src=\"./screenshot/final/result.png\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "## Limitted by the time given and our knowledge and experience in machine learning, although some improvements above can increase the training dataset, there is another problem we can never fix: the validation accuracy is decreasing when training accuracy is increasing. \n",
    "## This is mainly because the dataset is too small that each class only has 4 pictures. And in the 4 pictures, we have to divide into training, validation and test set. Besides, though we have to admit that keras is a user-friendly and straightforward framework, it has some drawbacks that all layer is encapsulated  well. The verbose mode is not enough for providing details that we can know what features that the cnn generated. \n",
    "## If more time is allowed, we might have more options to ameliorate our implementation.\n",
    "## Here are several possible options we considered:\n",
    "### 1. CNN-HMM Hybrid model\n",
    "### 2. Using edge detection to extract ear image from the original image to reduce noice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>https://blog.csdn.net/luanpeng825485697/article/details/80144300\n",
    "<li>https://forums.fast.ai/t/cnn-in-keras-overfitting-even-after-dropout-batch-normalization-and-augmentation/17994\n",
    "<li>https://blog.csdn.net/qq_41185868/article/details/79640111\n",
    "<li>https://machinelearningmastery.com/image-augmentation-deep-learning-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Ear Recognition\n",
    "Yifei Feng | U81901533\n",
    "\n",
    "Zehui Jiang | U68975915\n",
    "\n",
    "Guangxing Ren | U072735315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify your problem\n",
    "**Take a look at your topic and available resources, come up with a problem you think is interesting and worth working on. Give details about why is it a valuable topic and then describe the problem you are going to solve in a precise way.**\n",
    "\n",
    "#AI recognization system has been well implementing these days. A famous commercial example is facing ID by Apple. Before unlocking the device or finish the transaction, a facial scan is being conducted to check the identification of the user.  Another similar area#\n",
    "\n",
    "The process of precisely recognize people by ears also has been getting major attention in recent years. It represents an important step in the biometric research, especially as a complement to face recognition systems which have difficulty in real conditions. This is due to the great variation in shapes, variable lighting conditions, and the changing profile shape which is a planar representation of a complex object. We present an ear recognition system involving a convolutional neural network (CNN)  to identify a person given an input image.\n",
    "\n",
    "There are few methods \n",
    "Data:\n",
    "We are given a set of left ear image from people with different identities. For each ear(each person). Four images are given, we for ears “in the wild” (there is no constraint in the way of taking the photo), two for ear images taken with a “donut device” that serves as a background and somewhat controls lighting.  There are 4*195(individual ear) in the dataset.\n",
    "\n",
    " http://cs-people.bu.edu/wdqin/earImageDataset.zip\n",
    " \n",
    " \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backgroud\n",
    "\n",
    "\n",
    "Find out about and list more than one existing (possible) solution to your problem, make a brief comparison, and evaluation on these methods. Choose one of the existing solutions as a baseline for comparison.  Consider how much time you have and effort might be required to reproduce the result. Be realistic with your goal.\n",
    "\n",
    "\n",
    "We found an existing Face Recognition CNN that applies Convolutional Neural Network. \n",
    "\n",
    "We use kears as our open-source neural-network library.Keras is a high-level API for neural networks. It is written in Python and its biggest advantage is its ability to run on top of state-of-art deep learning libraries/frameworks such as TensorFlow, CNTK or Theano.\n",
    "\n",
    "\n",
    "\n",
    "These are the time distribution we planned for each individual member:\n",
    "\n",
    "Background research:   4 hours\n",
    "Reproducing Baseline: 6 hours\n",
    "Improvement: 5  hours\n",
    "Final Report: 4 hours\n",
    "\n",
    "We don’t want to ignore the importance of background research and report. Since we are given limited time for this complicated project. We want to spend more time on understanding the existing algorithm and application.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Reproducing the baseline:\n",
    "\n",
    "We found an existing Face Recognition CNN  that applies Convolutional Neural Network. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from skimage.io import imread_collection\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.utils import np_utils \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#loading dataset from directory and resize every picture to 200x200 dmension\n",
    "def load_data(path, size):\n",
    "    #creating a collection with the available images\n",
    "    image = imread_collection(path)\n",
    "    image_set = []\n",
    "    for n in image:\n",
    "        n = cv2.cvtColor(n,cv2.COLOR_RGB2GRAY)\n",
    "        n = cv2.resize(n,(size,size)) \n",
    "        n = n / 255\n",
    "        image_set.append(n)\n",
    "    return image_set \n",
    "    \n",
    "def set_init(dataset, train_set_ratio, valid_set_ratio, test_set_ratio):\n",
    "\n",
    "    \n",
    "    #creating label set for all images\n",
    "    label = np.empty(195*4)\n",
    "    for i in range(195):\n",
    "        label[i*4:i*4+4] = i\n",
    "    label = label.astype(np.int)\n",
    "    label = np_utils.to_categorical(label, 195)#transfer to one-hot matrix\n",
    "    \n",
    "    train_num = 780*train_set_ratio\n",
    "    train_num = int(train_num)\n",
    "    valid_num = 780*valid_set_ratio\n",
    "    valid_num = int(valid_num)\n",
    "    test_num = 780*test_set_ratio\n",
    "    test_num = int(test_num) \n",
    "    \n",
    "    train_data = np.empty((train_num,200,200))  #creating numpy array for different datasets\n",
    "    train_label = np.empty((train_num,195))   \n",
    "    valid_data = np.empty((valid_num, 200,200))   \n",
    "    valid_label = np.empty((valid_num,195))   \n",
    "    test_data = np.empty((test_num,200,200))  \n",
    "    test_label = np.empty((test_num,195)) \n",
    "    \n",
    "    x_test_tot = np.empty((valid_num + test_num,200,200))\n",
    "    y_test_tot = np.empty((valid_num + test_num,195))\n",
    "    \n",
    "    #split into train set and validation-test set\n",
    "    train_data, x_test_tot, train_label, y_test_tot = train_test_split(dataset, label, test_size = 1-train_set_ratio)\n",
    "    \n",
    "    #split validation-test set into validation and test set\n",
    "    valid_data, test_data, valid_label, test_label = train_test_split(x_test_tot, y_test_tot, test_size = test_set_ratio/(valid_set_ratio + test_set_ratio))\n",
    "    \n",
    "    train_data = np.asarray(train_data)\n",
    "    valid_data = np.asarray(valid_data)\n",
    "    test_data = np.asarray(test_data)\n",
    "    result = [(train_data, train_label), (valid_data, valid_label),(test_data, test_label)]\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = load_data('original/*.jpg',200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = set_init(data_set, 0.8, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from PIL import Image\n",
    "def train(data, batch_size, epochs, nb_filters, pool_size, kernel_size):\n",
    "    np.random.seed(1337)  # for reproducibility\n",
    "    img_rows, img_cols = 200, 200  # width and height of pictures\n",
    "    nb_classes = 195  # number of classes\n",
    "    input_shape = (img_rows, img_cols,1)  # dimenstion\n",
    "\n",
    "    [(X_train, Y_train), (X_valid, Y_valid),(X_test, Y_test)] = data\n",
    "\n",
    "    X_train = X_train[:,:,:,np.newaxis]  # add one dimenstion, keras required. total 4 dimension.\n",
    "    X_valid=X_valid[:,:,:,np.newaxis]  \n",
    "    X_test=X_test[:,:,:,np.newaxis]  \n",
    "    print('dimension of train set：', X_train.shape,Y_train.shape)\n",
    "    print('dimension of test set：', X_test.shape,Y_test.shape)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6,kernel_size,input_shape=input_shape,strides=1))  # 卷积层1\n",
    "    model.add(AveragePooling2D(pool_size,strides=2))  # 池化层\n",
    "    model.add(Conv2D(12,kernel_size,strides=1))  # 卷积层2\n",
    "    model.add(AveragePooling2D(pool_size,strides=2))  # 池化层\n",
    "    model.add(Flatten())  # 拉成一维数据\n",
    "    model.add(Dense(nb_classes))  # 全连接层2\n",
    "    model.add(Activation('sigmoid'))  # sigmoid评分\n",
    "\n",
    "    # 编译模型\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\n",
    "    # 训练模型\n",
    "    model.fit(X_train, Y_train, batch_size, epochs,verbose=1, validation_data=(X_valid, Y_valid))\n",
    "    # 评估模型\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = y_pred.argmax(axis=1)   # 获取概率最大的分类，获取每行最大值所在的列\n",
    "    for i in range(len(y_pred)):\n",
    "    #     oneimg = X_test[i,:,:,0]*256\n",
    "    #     im = Image.fromarray(oneimg)\n",
    "    #     im.show()\n",
    "        print('第%d个人识别为第%d个人'%(Y_test.argmax(axis=1).item(i),y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数据集的维度： (624, 200, 200, 1) (624, 195)\n",
      "测试数据集的维度： (78, 200, 200, 1) (78, 195)\n",
      "Train on 624 samples, validate on 78 samples\n",
      "Epoch 1/30\n",
      "624/624 [==============================] - 1s 1ms/step - loss: 5.4536 - acc: 0.0048 - val_loss: 5.2910 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "624/624 [==============================] - 1s 890us/step - loss: 5.2756 - acc: 0.0176 - val_loss: 5.4804 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "624/624 [==============================] - 1s 921us/step - loss: 5.2198 - acc: 0.0385 - val_loss: 5.4052 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "624/624 [==============================] - 1s 926us/step - loss: 5.1425 - acc: 0.0769 - val_loss: 5.5298 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "624/624 [==============================] - 1s 914us/step - loss: 5.0097 - acc: 0.1218 - val_loss: 5.9510 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "624/624 [==============================] - 1s 882us/step - loss: 4.8715 - acc: 0.1282 - val_loss: 5.8096 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "624/624 [==============================] - 1s 869us/step - loss: 4.6691 - acc: 0.1859 - val_loss: 6.1287 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "624/624 [==============================] - 1s 872us/step - loss: 4.3007 - acc: 0.2340 - val_loss: 7.6088 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "624/624 [==============================] - 1s 952us/step - loss: 4.2188 - acc: 0.2821 - val_loss: 6.2276 - val_acc: 0.0128\n",
      "Epoch 10/30\n",
      "624/624 [==============================] - 1s 940us/step - loss: 4.0660 - acc: 0.3686 - val_loss: 7.9962 - val_acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "624/624 [==============================] - 1s 947us/step - loss: 3.1928 - acc: 0.4776 - val_loss: 5.5994 - val_acc: 0.0128\n",
      "Epoch 12/30\n",
      "624/624 [==============================] - 1s 921us/step - loss: 3.3628 - acc: 0.4679 - val_loss: 7.7965 - val_acc: 0.0256\n",
      "Epoch 13/30\n",
      "624/624 [==============================] - 1s 886us/step - loss: 2.4455 - acc: 0.6330 - val_loss: 8.4654 - val_acc: 0.0128\n",
      "Epoch 14/30\n",
      "624/624 [==============================] - 1s 883us/step - loss: 1.4953 - acc: 0.6891 - val_loss: 6.8011 - val_acc: 0.0256\n",
      "Epoch 15/30\n",
      "624/624 [==============================] - 1s 864us/step - loss: 1.5865 - acc: 0.7404 - val_loss: 10.0958 - val_acc: 0.0128\n",
      "Epoch 16/30\n",
      "624/624 [==============================] - 1s 952us/step - loss: 0.8300 - acc: 0.8333 - val_loss: 10.2083 - val_acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "624/624 [==============================] - 1s 890us/step - loss: 0.7638 - acc: 0.8814 - val_loss: 10.4910 - val_acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "624/624 [==============================] - 1s 874us/step - loss: 0.6151 - acc: 0.8990 - val_loss: 8.8080 - val_acc: 0.1282\n",
      "Epoch 19/30\n",
      "624/624 [==============================] - 1s 900us/step - loss: 0.0781 - acc: 0.9872 - val_loss: 8.9248 - val_acc: 0.1282\n",
      "Epoch 20/30\n",
      "624/624 [==============================] - 1s 933us/step - loss: 0.0444 - acc: 0.9920 - val_loss: 8.6891 - val_acc: 0.1410\n",
      "Epoch 21/30\n",
      "624/624 [==============================] - 1s 920us/step - loss: 0.0335 - acc: 0.9952 - val_loss: 8.6782 - val_acc: 0.1410\n",
      "Epoch 22/30\n",
      "624/624 [==============================] - 1s 915us/step - loss: 0.0235 - acc: 0.9984 - val_loss: 8.7004 - val_acc: 0.1410\n",
      "Epoch 23/30\n",
      "624/624 [==============================] - 1s 884us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 8.7436 - val_acc: 0.1410\n",
      "Epoch 24/30\n",
      "624/624 [==============================] - 1s 874us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 8.7507 - val_acc: 0.1410\n",
      "Epoch 25/30\n",
      "624/624 [==============================] - 1s 947us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 8.7723 - val_acc: 0.1410\n",
      "Epoch 26/30\n",
      "624/624 [==============================] - 1s 946us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 8.8143 - val_acc: 0.1410\n",
      "Epoch 27/30\n",
      "624/624 [==============================] - 1s 949us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 8.8238 - val_acc: 0.1410\n",
      "Epoch 28/30\n",
      "624/624 [==============================] - 1s 969us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 8.8462 - val_acc: 0.1410\n",
      "Epoch 29/30\n",
      "624/624 [==============================] - 1s 999us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 8.8682 - val_acc: 0.1410\n",
      "Epoch 30/30\n",
      "624/624 [==============================] - 1s 963us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 8.8905 - val_acc: 0.1410\n",
      "Test score: 8.397333414126665\n",
      "Test accuracy: 0.16666666781290984\n",
      "第36个人识别为第67个人\n",
      "第2个人识别为第158个人\n",
      "第104个人识别为第63个人\n",
      "第105个人识别为第38个人\n",
      "第137个人识别为第146个人\n",
      "第124个人识别为第152个人\n",
      "第20个人识别为第178个人\n",
      "第180个人识别为第180个人\n",
      "第172个人识别为第47个人\n",
      "第184个人识别为第184个人\n",
      "第27个人识别为第27个人\n",
      "第35个人识别为第159个人\n",
      "第189个人识别为第50个人\n",
      "第82个人识别为第82个人\n",
      "第48个人识别为第39个人\n",
      "第24个人识别为第174个人\n",
      "第143个人识别为第27个人\n",
      "第14个人识别为第173个人\n",
      "第64个人识别为第64个人\n",
      "第19个人识别为第18个人\n",
      "第151个人识别为第179个人\n",
      "第131个人识别为第92个人\n",
      "第33个人识别为第114个人\n",
      "第50个人识别为第190个人\n",
      "第39个人识别为第186个人\n",
      "第0个人识别为第54个人\n",
      "第187个人识别为第76个人\n",
      "第35个人识别为第77个人\n",
      "第45个人识别为第99个人\n",
      "第35个人识别为第28个人\n",
      "第3个人识别为第145个人\n",
      "第55个人识别为第147个人\n",
      "第69个人识别为第182个人\n",
      "第8个人识别为第100个人\n",
      "第179个人识别为第71个人\n",
      "第4个人识别为第82个人\n",
      "第49个人识别为第153个人\n",
      "第21个人识别为第21个人\n",
      "第96个人识别为第168个人\n",
      "第89个人识别为第92个人\n",
      "第46个人识别为第11个人\n",
      "第80个人识别为第69个人\n",
      "第13个人识别为第108个人\n",
      "第169个人识别为第71个人\n",
      "第60个人识别为第178个人\n",
      "第15个人识别为第87个人\n",
      "第104个人识别为第33个人\n",
      "第86个人识别为第169个人\n",
      "第151个人识别为第179个人\n",
      "第139个人识别为第39个人\n",
      "第96个人识别为第158个人\n",
      "第113个人识别为第174个人\n",
      "第43个人识别为第51个人\n",
      "第60个人识别为第63个人\n",
      "第31个人识别为第5个人\n",
      "第143个人识别为第79个人\n",
      "第138个人识别为第138个人\n",
      "第106个人识别为第106个人\n",
      "第57个人识别为第57个人\n",
      "第110个人识别为第110个人\n",
      "第111个人识别为第37个人\n",
      "第146个人识别为第146个人\n",
      "第104个人识别为第17个人\n",
      "第75个人识别为第66个人\n",
      "第97个人识别为第97个人\n",
      "第107个人识别为第153个人\n",
      "第138个人识别为第72个人\n",
      "第59个人识别为第3个人\n",
      "第61个人识别为第153个人\n",
      "第30个人识别为第108个人\n",
      "第98个人识别为第98个人\n",
      "第81个人识别为第168个人\n",
      "第175个人识别为第179个人\n",
      "第84个人识别为第45个人\n",
      "第185个人识别为第142个人\n",
      "第46个人识别为第17个人\n",
      "第109个人识别为第77个人\n",
      "第90个人识别为第68个人\n"
     ]
    }
   ],
   "source": [
    "train(data, 100, 30, 64, 4, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement\n",
    "## 1. Regularization\n",
    "## 2. Augmentation\n",
    "## 3. Crop\n",
    "## 4. k-fold\n",
    "## 5. Dropout\n",
    "## 6. Compromise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = load_data('original/*.jpg')\n",
    "data_set = process_image(data_set, size = 200, margin = [500,2500,500,3500])\n",
    "data_set = set_init(data, train_set_ratio = 1, valid_set_ratio = 0, test_set_ratio = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "from keras import regularizers\n",
    "from keras.utils import generic_utils\n",
    "\n",
    "def train(data, batch_size, epochs, nb_filters, pool_size, kernel_size):\n",
    "    np.random.seed(1337)  # for reproducibility\n",
    "    img_rows, img_cols = 200, 200  # 输入图片样本的宽高\n",
    "    nb_classes = 195  # 分类数目\n",
    "    input_shape = (img_rows, img_cols,1)  # 输入图片的维度\n",
    "\n",
    "    [(X_train, Y_train), (X_valid, Y_valid),(X_test, Y_test)] = data\n",
    "    X_train = X_train[:,:,:,np.newaxis]  # 添加一个维度，代表图片通道。这样数据集共4个维度，样本个数、宽度、高度、通道数\n",
    "    print('样本数据集的维度：', X_train.shape,Y_train.shape)\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,kernel_size,padding = 'same', input_shape=input_shape,strides=1))  # 卷积层1\n",
    "    model.add(Activation('relu'))  \n",
    "    model.add(AveragePooling2D(pool_size,strides=2))  # 池化层  \n",
    "    model.add(Conv2D(64,kernel_size,strides=1))  # 卷积层2\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(AveragePooling2D(pool_size,strides=2))  # 池化层\n",
    "    model.add(Flatten())  # 拉成一维数据\n",
    "    model.add(Dense(nb_classes)) # 全连接层2, ,kernel_regularizer=regularizers.l2(0.05)\n",
    "    model.add(Activation('softmax'))  # softmax评分\n",
    "\n",
    "\n",
    "    # 编译模型\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\n",
    "    # 训练模型\n",
    "    model.fit(X_train, Y_train, batch_size, epochs,verbose=1)\n",
    "    \n",
    "    \n",
    "    y_out = model.predict(X_target)\n",
    "    y_out = y_out.argmax(axis=1)\n",
    "    print('This is person No.%d'%(y_out))\n",
    "    for i in range(len(y_train_out)):\n",
    "        print('train: 第%d个人识别为第%d个人'%(Y_train.argmax(axis=1).item(i),y_train_out[i]))\n",
    "        \n",
    "\n",
    "    y_pred = model.predict(X_train)\n",
    "    y_pred = y_pred.argmax(axis=1)   # 获取概率最大的分类，获取每行最大值所在的列\n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_pred[i] == y_out):\n",
    "            print('This is a picture from the same peroson')\n",
    "            X_train[i] = X_train[i] * 255\n",
    "            plt.imshow(X_train[i], cmap = 'gray')\n",
    "            plt.show()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
